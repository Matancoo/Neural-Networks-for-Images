{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX2_autoencoders_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Auto-Encoding. Define a convolutional autoencoder to encode (and decode) the images into a small dimensional latent space d (around d=10) containing no spatial structure (1x1xd tensors). Explore the reconstruction error over the test set when (i): using low and high latent space dimension d, and (ii) using a fixed d yet different architecture with more and less layers (/weights). Report these tests and the best score obtained (i.e. plot your results as a function of the different experiments performed). The best practice of implementing this code is by defining an Encoder and a Decoder as separate Pytorch modules.\n",
        "\n"
      ],
      "metadata": {
        "id": "w0Q_DItXQVgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bScrFopwQgbx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dqqsntVUQ8ZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # this module is useful to work with numerical arrays\n",
        "import pandas as pd \n",
        "import random \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "from torch.nn.modules.conv import ConvTranspose2d\n",
        "from torch.nn.modules.flatten import Unflatten\n",
        "from os import truncate\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.nn.modules.container import Sequential\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "batch_size =16\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2) \n"
      ],
      "metadata": {
        "id": "BoDdZV9kSQ_Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance of data"
      ],
      "metadata": {
        "id": "_ZCAg6rHaYx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total = 0\n",
        "# counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
        "\n",
        "\n",
        "# for data in trainloader:\n",
        "#     Xs, ys = data\n",
        "#     for y in ys:\n",
        "#         counter_dict[int(y)] += 1\n",
        "#         total += 1\n",
        "\n",
        "# print(counter_dict)\n",
        "\n",
        "# for i in counter_dict:\n",
        "#     print(f\"{i}: {counter_dict[i]/total*100.0}%\")"
      ],
      "metadata": {
        "id": "wzFPQ3xjY-Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining encoder and decorder classes\n"
      ],
      "metadata": {
        "id": "4VhEvJvRdAON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.conv import ConvTranspose2d\n",
        "from torch.nn.modules.flatten import Unflatten\n",
        "from os import truncate\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.nn.modules.container import Sequential\n",
        "\n",
        "#BASELINE ENCODER MODEL \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,latenSpace_dim):\n",
        "        super().__init__()\n",
        "        #conv\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "          nn.Conv2d(1,32,3,stride =2, padding = 1),\n",
        "          nn.ReLU(True),\n",
        "          nn.Conv2d(32, 64, 3, stride=2, padding = 1),\n",
        "          nn.BatchNorm2d(num_features=64),\n",
        "          nn.ReLU(True),                                 #relu before batch noormalization\n",
        "          nn.Conv2d(64,128,3, stride =2, padding = 0),  #reduce quite drastically the image (3 by 3) before using fC layers to get vector\n",
        "          nn.ReLU(True)\n",
        "        )\n",
        "         \n",
        "        #flattern\n",
        "        self.flat = nn.Flatten(start_dim=1)\n",
        "        \n",
        "        #linear  \n",
        "        self.encoder_lin = nn.Sequential(\n",
        "          nn.Linear(128 * 3 * 3,128), \n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(128,latenSpace_dim) \n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.encoder_cnn(x)\n",
        "        x = self.flat(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "\n",
        "# BASELINE DECODER  --> symetric to encoder model\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,latenSpace_dim):\n",
        "      super().__init__()\n",
        "      #linear \n",
        "      self.decoder_lin = nn.Sequential(\n",
        "        nn.Linear(latenSpace_dim,128),\n",
        "        nn.ReLU(True), \n",
        "        nn.Linear(128,128*3*3), \n",
        "        nn.ReLU(True)\n",
        "      )\n",
        "      #unflattern\n",
        "      self.unflat = nn.Unflatten(dim=1, unflattened_size=(128,3,3))\n",
        "      \n",
        "      #conv\n",
        "      #here I inversed the batch_norm and relu layers and added one batch norm\n",
        "      self.decoder_cnn = nn.Sequential(\n",
        "        nn.ConvTranspose2d(128,64,3,stride=2, padding= 0, output_padding =0), #why output padding is zero here? what does outputpadding does?\n",
        "        nn.BatchNorm2d(num_features=64),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(64,32,3,stride =2, padding =1, output_padding =1),\n",
        "        nn.BatchNorm2d(num_features = 32),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.decoder_lin(x)\n",
        "      x = self.unflat(x)\n",
        "      x = self.decoder_cnn(x)\n",
        "      return x\n",
        "\n",
        "      \n",
        "        "
      ],
      "metadata": {
        "id": "CpJOCrBpdFEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "d= 5\n",
        "encoder = Encoder(latenSpace_dim=5)\n",
        "decoder = Decoder(latenSpace_dim=5)\n",
        "\n",
        "parameters_to_opt = [                                                            #how do i do it the iterable way? see adam doc\n",
        "    {'params': encoder.parameters()},\n",
        "    {'params': decoder.parameters()}\n",
        "]\n",
        "\n",
        "\n",
        "loss_funk = torch.nn.HuberLoss()                                                       \n",
        "optimizer = optim.Adam(parameters_to_opt,lr=0.016,weight_decay=1e-05)         #what is weight decay??\n"
      ],
      "metadata": {
        "id": "-gN2FTyZ1wou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(encoder, decoder,dataloader,loss_funk,optimizer):\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  losses = 0\n",
        "  for batch in trainloader:\n",
        "    data, true_labels = batch\n",
        "    #Forward pass\n",
        "    new_data = decoder(encoder(data))        \n",
        "\n",
        "    #Calculating loss\n",
        "    loss = loss_funk(data,new_data)   \n",
        "\n",
        "    #Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses += loss.item()\n",
        "  losses = losses/len(trainloader)\n",
        "  return losses\n",
        "\n",
        "\n",
        "def test_epoch(encoder, decoder,dataloader,loss_function,optimizer):\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "  losses = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in testloader:\n",
        "      data, true_labels = batch\n",
        "      new_data = decoder(encoder(data)) \n",
        "      loss = loss_funk(data,new_data)                                        \n",
        "      losses += loss.item()\n",
        "  losses = losses/len(testloader)\n",
        "  return losses\n",
        "  \n"
      ],
      "metadata": {
        "id": "JzZF8dMM2E1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EPOCH = 7\n",
        "train_loss =[]\n",
        "test_loss =[]\n",
        "epoch_number =0\n",
        "\n",
        "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
        "  #training\n",
        "  epoch_train_loss = train_epoch(encoder,decoder,trainloader,loss_funk,optimizer)\n",
        "  train_loss.append(epoch_train_loss)\n",
        "\n",
        "  #testing\n",
        "  epoch_test_loss = test_epoch(encoder,decoder,testloader,loss_funk,optimizer)\n",
        "  test_loss.append(epoch_test_loss)\n",
        "  epoch_number += 1\n",
        "  print('epoch number :'+ str(epoch_number))\n",
        "  \n",
        "  #plotting last losses\n",
        "  if epoch == EPOCH-1:\n",
        "    print(epoch_train_loss)\n",
        "    print(epoch_test_loss)\n",
        "  \n",
        "#plotting training and testing loss\n",
        "x = np.linspace(1,EPOCH,EPOCH).astype(int)\n",
        "y_train = np.array(train_loss)\n",
        "y_test = np.array(test_loss)\n",
        "\n",
        "plt.plot(x,y_train, 'r')\n",
        "plt.plot(x,y_test,'b')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "2KKD0iJFZcpv",
        "outputId": "9b28edaa-8be5-45a1-f3f9-12b01a733675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235\n",
            "epoch number :1\n",
            "235\n",
            "epoch number :2\n",
            "235\n",
            "epoch number :3\n",
            "235\n",
            "epoch number :4\n",
            "235\n",
            "epoch number :5\n",
            "235\n",
            "epoch number :6\n",
            "235\n",
            "epoch number :7\n",
            "0.013583975804454468\n",
            "0.013396198675036431\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpolation. Once you have a trained the AE, you can use it to interpolate between two digits in latent space. That is, let I1 and I2 be two different digits, perform the interpolation D((E(I1)*)+(E(I2)*(1-))) for a=0...1 where D denotes the decoder and E the encoder. (i) Include the gradual images obtained. (ii) Try different pairs of digits. (iii) Try repeating this operation with an AE trained using a higher embedding dimension d~20. (iv) Which is better? Provide an explanation why the quality increased or decreased."
      ],
      "metadata": {
        "id": "wZ0zyfOqQoWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#interpolation check of our AE\n",
        "#sample two images from data\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from skimage import io \n",
        "import plotly.express as px\n",
        "\n",
        "def interpolate(encoder,decoder,im1,im2,alpha):\n",
        "  laten_vec1 = encoder(im1)\n",
        "  laten_vec2 = encoder(im2)\n",
        "  interpolation = decoder((laten_vec1 * alpha) + (laten_vec2 * (1-alpha)))\n",
        "  return interpolation\n",
        "\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "for batch in testloader:\n",
        "  data, labels = batch\n",
        "  break\n",
        "\n",
        "\n",
        "im1, im2  = data[13], data[21]        \n",
        "im1, im2 = torch.unsqueeze(im1,0), torch.unsqueeze(im2,0)    \n",
        "                                       #chose two images from batch size (batch size is shuffled)\n",
        "\n",
        "                                                  #chose 10 alph values linearly spaced\n",
        "new_images = []\n",
        "for alpha in np.linspace(0,1,10):  \n",
        "  new_image = interpolate(encoder,decoder,im1,im2,alpha).detach().squeeze().numpy()\n",
        "  new_images.append(new_image)\n",
        "\n",
        " # reconstruct images from the chosen laten vectors\n",
        "\n",
        "\n",
        "\n",
        "im1 = im1.detach().numpy().squeeze()\n",
        "im2 = im2.detach().numpy().squeeze()\n",
        "initial_ims = np.array([im1,im2])\n",
        "\n",
        "new_images = np.array(new_images)\n",
        "print(len(new_images))\n",
        "print(new_images[0].dtype)\n",
        "fig1 = px.imshow(new_images, facet_col=0)\n",
        "fig2 = px.imshow(initial_ims,facet_col=0)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "           \n",
        "           "
      ],
      "metadata": {
        "id": "pRhMSDm8ouyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decorrelation. In the following experiment we will investigate the connection between dimensional reduction and dependencies (/redundancies) in the representation. Carry this out by computing the Pearson correlations between different coordinates of the latent codes (based on a few thousands encoded images), and use them to come up with a single value (that you came up with!) for measuring the overall correlation. Plot this value with respect to the latent space dimension d (over at least 4 values of d). Explain your choices and the trend in correlation versus d that you observe. Provide an explanation in your report.\n"
      ],
      "metadata": {
        "id": "-5KMBjsqQ0Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import cov\n",
        "#Decorrelation\n",
        "#dimensional reduction Vs dependencies in laten-Space\n",
        "from scipy.linalg import null_space\n",
        "COUNT = 256\n",
        "#method 1\n",
        "# encoder.eval()\n",
        "# with torch.no_grad():\n",
        "#   count =0 \n",
        "#   laten_vecs = []\n",
        "#   for batch in testset:\n",
        "#     image,_ = batch\n",
        "#     image = image[None,:]\n",
        "#     laten_vecs.append(np.array(encoder(image)))\n",
        "#     count+=1\n",
        "#     if count ==COUNT:\n",
        "#       break\n",
        "\n",
        "# laten_vecs = np.array(laten_vecs).squeeze()\n",
        "# print(laten_vecs.shape)\n",
        "\n",
        "\n",
        "#method 2\n",
        "encoder.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in testloader:\n",
        "    data,_ = batch\n",
        "    laten_vecs = np.array(encoder(data))\n",
        "    break  \n",
        "\n",
        "\n",
        "print(laten_vecs.shape)\n",
        "\n",
        "corr_matrix = np.corrcoef(laten_vecs)\n",
        "\n",
        "print(corr_matrix.shape)\n",
        "\n",
        "#method 0 - Take mean of rows for each matrix in array. we then take again the mean.\n",
        "new_matrix = corr_matrix - (np.eye(COUNT) * np.diag(corr_matrix))\n",
        "mean_rows_vec = np.mean(new_matrix, axis = 0)\n",
        "mean_corr_estimate = np.mean(mean_rows_vec) \n",
        "print('mean_corr_estimate: ' + str(mean_corr_estimate))                                                                   #mean 1  - np.eye(256)@ np.diag(x)\n",
        "\n",
        "\n",
        "#method 1 -eigen value of zero tells us that there is linear dependency and thus redudencies in the data. (look at eeigen space of zero)\n",
        "\n",
        "cov_matrix = np.cov(laten_vecs)\n",
        "null_dim = null_space(cov_matrix).shape[1]\n",
        "print('dim of null space: '+ str(null_dim))\n",
        "\n",
        "# #method 2 - calculate the determinant of the matrix \n",
        "det_corr = np.linalg.det(corr_matrix)\n",
        "print('det of correlation matrix:'+ str(det_corr))\n",
        "\n",
        "\n",
        "# #method 3 - orthogonally project each column of matrix onto the same space and calculate mean/corr on these vectors\n",
        "  \n",
        "\n",
        "# EPOCH =2\n",
        "# def get_all_laten_vectors(encoder,testloader):\n",
        "#   encoder.eval()\n",
        "#   total_laten_vectors = []\n",
        "#   with torch.no_grad():\n",
        "#     for batch in testloader:\n",
        "#       data,_ = batch\n",
        "#       laten_vectors = np.array(encoder(data))\n",
        "#       total_laten_vectors.append(laten_vectors)\n",
        "#       break  #TODO: how to run multiple times? when i remove break i get only 16 samples\n",
        "      \n",
        "#   return laten_vectors # lists of images\n",
        "  \n",
        "\n",
        "# laten_vectors = get_all_laten_vectors(encoder,testloader)\n",
        "# print(laten_vectors.shape\n",
        "\n",
        "\n",
        "#TODO: cant understad why this is wrong !!\n",
        "# encoder.eval()\n",
        "# with torch.no_grad():\n",
        "#   for data in testloader:\n",
        "#     images, labels = data\n",
        "#     laten_vecs = np.array(encoder(images)).T\n",
        "#     break\n",
        "# print(laten_vecs.shape)\n",
        "# laten_vecs = np.apply_along_axis(lambda x: np.corrcoef(x),axis= 0, arr= laten_vecs)\n",
        "# print(laten_vecs.shape)\n",
        "# print(laten_vecs[0].shape)\n",
        "# #method 0 - Take mean of rows for each matrix in array. we then take again the mean. \n",
        "# # new_matrix = corr_matrix - np.eye(256)* np.diag(corr_matrix)  \n",
        "\n",
        "# mean_vec = np.apply_along_axis(lambda x: np.mean(x),axis= 0, arr= laten_vecs) #mean 1  - np.eye(256)@ np.diag(x)\n",
        "# mean_vec = np.apply_along_axis(lambda x: np.mean(x),axis= 0, arr= mean_vec) #mean2 represents an array with the internal correlation of each of the laten vectors extracted.\n",
        "\n",
        "\n",
        "# #method 1 -eigen value of zero tells us that there is linear dependency and thus redudencies in the data. (look at eeigen space of zero)\n",
        "\n",
        "# dim_vec = np.apply_along_axis(lambda x: np.cov(x),axis= 0, arr= laten_vecs)\n",
        "# null_dim = np.apply_along_axis(lambda x: len(null_space(x)),axis= 0, arr= laten_vecs)\n",
        "\n",
        "# #method 2 - calculate the determinant of the matrix \n",
        "# det_corr_vec = np.apply_along_axis(lambda x: np.linalg.det(x),axis= 0, arr= laten_vecs)\n",
        "# print('det:'+ str(det_corr_vec[0]))\n",
        "\n",
        "\n",
        "# #method 3 - orthogonally project each column of matrix onto the same space and calculate mean/corr on these vectors\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# corr_matrix = np.corrcoef(laten_vectors)\n",
        "\n",
        "# #method 0 - Take mean of either rows or columns (corr is symmetric) to get an array. Take mean of subsequent array.\n",
        "# # new_matrix = corr_matrix - np.eye(256)* np.diag(corr_matrix)  \n",
        "# mean_array = np.mean(new_matrix,axis=0) #mean of each row \n",
        "# mean_corr_val = np.mean(mean_array, axis =0)\n",
        "# print(mean_corr_val)\n",
        "\n",
        "# #method 1 -eigen value of zero tells us that there is linear dependency and thus redudencies in the data. (look at eeigen space of zero)\n",
        "# cov_matrix = np.cov(laten_vectors)\n",
        "# null_dim = len(null_space(cov_matrix))\n",
        "# print('dim of null space: '+ str(null_dim ))\n",
        "\n",
        "# #method 2 - calculate the determinant of the matrix \n",
        "# #The determinant of the correlation matrix will equal 1.0 only if all correlations equal 0. Otherwise the determinant will be less than 1.\n",
        "\n",
        "# det = np.linalg.det(corr_matrix)\n",
        "# print('det:'+ str(det)) #TODO: dont get why det is zero and null sapce 256 !!!\n",
        "\n",
        "\n",
        "# #method 3 - orthogonally project each column of matrix onto the same space and calculate mean/corr on these vectors\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "RcMN-ZAeeJaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jKsYaq86-rEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "osADiRHR-qvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer Learning. Use a pre-trained encoder as a fixed network (i.e. excuse its weights from the following training optimization), and attach it to a small MLP (in latent space) and train only the latter to classify the digits (recall that MNIST is labeled). Do this with a small fraction of the labels in the dataset (~tens of images). Compare the performance of this solution next to the option of training the entire network (i.e., allow the encoder weights to train as well as the classification MLP) over this small number of training examples. Note that both training schemes used the same number of labels. Which option operated best? Report all the choices made (e.g., latent space dimension, MLP arch., and number of labels used, etc.), and the results obtained by each of the two training approaches."
      ],
      "metadata": {
        "id": "4OHFMLlG-m4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transfer learning:\n",
        "# #Encoder FatMan model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,latenSpace_dim):\n",
        "        super().__init__()\n",
        "        #conv\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "          nn.Conv2d(1,32,3,stride =2, padding = 1),\n",
        "          nn.ReLU(True),\n",
        "          nn.Conv2d(32, 64, 3, stride=2, padding = 1),\n",
        "          nn.BatchNorm2d(num_features=64),\n",
        "          nn.ReLU(True),                                 #relu before batch noormalization\n",
        "          nn.Conv2d(64,128,3, stride =2, padding = 0),  #reduce quite drastically the image (3 by 3) before using fC layers to get vector\n",
        "          nn.ReLU(True)\n",
        "        )\n",
        "         \n",
        "        #flattern\n",
        "        self.flat = nn.Flatten(start_dim=1)\n",
        "        \n",
        "        #linear  \n",
        "        self.encoder_lin = nn.Sequential(\n",
        "          nn.Linear(128 * 3 * 3,128), \n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(128,latenSpace_dim) \n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.encoder_cnn(x)\n",
        "        x = self.flat(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "\n",
        "#creating a small Multi Layer Perceptron \n",
        "\n",
        "\n",
        "#helper function\n",
        "def get_subset(size,testset):\n",
        "  train_set =[] #set of batchs based on batchsize\n",
        "  count =0 \n",
        "  for batch in testloader:\n",
        "   data, labels = batch[0], batch[1]\n",
        "   train_data = data[:size,:,:,:]\n",
        "   train_labels = labels[:size]\n",
        "  return [train_data,train_labels]\n",
        "\n",
        "\n",
        "EPOCH = 10\n",
        "LATEN_DIM =5 #size of laten space \n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,latenSpace_dim):\n",
        "        super().__init__()\n",
        "          #linear \n",
        "        self.MLP_lin = nn.Sequential(\n",
        "          nn.Linear(latenSpace_dim,32),\n",
        "          nn.ReLU(True), \n",
        "          nn.Linear(32,64), \n",
        "          nn.ReLU(True),\n",
        "          ####### symmetric side from decoder FatMan\n",
        "          nn.Linear(64,64), \n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(64,10))\n",
        "   \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.MLP_lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#calling models, loss function, optimizer, \n",
        "\n",
        "\n",
        "# encoder = Encoder(latenSpace_dim=4)\n",
        "mlp = MLP(latenSpace_dim=LATEN_DIM)\n",
        "encoder = Encoder(latenSpace_dim=LATEN_DIM)\n",
        "parameters_to_opt = [                                                        \n",
        "    {'params1': encoder.parameters()},\n",
        "    {'params2': mlp.parameters()}]\n",
        "    \n",
        "loss_funk = nn.CrossEntropyLoss()                                               \n",
        "optimizer = optim.Adam(params = encoder.parameters(),lr=0.0004,weight_decay=1e-05) #divided lr by 4 as batxh size =16\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(mlp, decoder,dataloader,loss_funk,optimizer):\n",
        "  encoder.train\n",
        "  mlp.train()\n",
        "  losses = 0\n",
        "  for batch in trainloader:\n",
        "    data, true_labels = batch\n",
        "    #Forward pass\n",
        "    labels = mlp(encoder(data))        \n",
        "\n",
        "    #Calculating loss\n",
        "    loss = loss_funk(labels,true_labels)   \n",
        "\n",
        "    #Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses += loss.item()\n",
        "  losses = losses/len(trainloader)\n",
        "  return losses\n",
        "\n",
        "\n",
        "def test_epoch(mlp, decoder,dataloader,loss_function,optimizer):\n",
        "  encoder.eval()\n",
        "  mlp.eval()\n",
        "  losses = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      data, true_labels = batch\n",
        "      labels = mlp(encoder(data)) \n",
        "      #Calculating loss\n",
        "      loss = loss_funk(labels,true_labels)                                        \n",
        "      losses += loss.item()\n",
        "  losses = losses/len(dataloader)\n",
        "  return losses\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        " \n",
        "train_loss =[]\n",
        "test_loss =[]\n",
        "epoch_number =0\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  #training\n",
        "  epoch_train_loss = train_epoch(mlp,encoder,trainloader,loss_funk,optimizer)\n",
        "  train_loss.append(epoch_train_loss)\n",
        "\n",
        "  #testing\n",
        "  epoch_test_loss = test_epoch(mlp,encoder,testloader,loss_funk,optimizer)\n",
        "  test_loss.append(epoch_test_loss)\n",
        "  epoch_number += 1\n",
        "  print('epoch number :'+ str(epoch_number))\n",
        "  \n",
        "  #plotting last losses\n",
        "  if epoch == EPOCH-1:\n",
        "    print(epoch_train_loss)\n",
        "    print(epoch_test_loss)\n",
        "  \n",
        "#plotting training and testing loss\n",
        "x = np.linspace(1,EPOCH,EPOCH).astype(int)\n",
        "y_train = np.array(train_loss)\n",
        "y_test = np.array(test_loss)\n",
        "\n",
        "plt.plot(x,y_train, 'r')\n",
        "plt.plot(x,y_test,'b')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "FvrE3aKw4a_4",
        "outputId": "cb14a93b-b899-45c4-9e9d-50744e6a6671"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch number :1\n",
            "epoch number :2\n",
            "epoch number :3\n",
            "epoch number :4\n",
            "epoch number :5\n",
            "epoch number :6\n",
            "epoch number :7\n",
            "epoch number :9\n",
            "epoch number :10\n",
            "0.5832924279178444\n",
            "0.6219805637762882\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    }
  ]
}